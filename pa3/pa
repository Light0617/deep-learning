1.yes

2.
What is the loss on the training data for that test run? Write your answer with at least 5 digits after the decimal point.
2.302585


3.
What is the training data loss that that run reports at the end? Use at least 5 digits after the decimal point.
2.303441


4.
Let's try a variety of learning rates, to find out which works best. We'll try 0.002, 0.01, 0.05, 0.2, 1.0, 5.0, and 20.0. We'll try all of those both without momentum (i.e. momentum=0.0 in the program) and with momentum (i.e. momentum=0.9 in the program), so we have a total of 7 x 2 = 14 experiments to run. Remember, what we're interested in right now is the loss on the training data, because that shows how well the optimization works.
Which of those 14 worked best?
Was the best run a run with momentum or without momentum?

The best of those 14 runs was with momentum.

m = 0
0.002 
2.304338
0.01
2.302117
0.05
2.292967
0.2
2.228969
1
1.598844
5
2.301322
20
2.302585

m = 0.9
0.002
2.300135
0.01
2.284022
0.05
2.008606
0.2
1.083429
1
2.018723
5
2.302585
20
2.302585


5.
What was the learning rate for the best of those 14 runs?

0.2

6.
We'll start with zero weight decay, 200 hidden units, 1000 optimization iterations, a learning rate of 0.35, momentum of 0.9, no early stopping, and mini-batch size 100, i.e. run a3(0, 200, 1000, 0.35, 0.9, false, 100). This run will take more time.
What is the validation data classification loss now? Write your answer with at least 5 digits after the decimal point.
validation data is 0.430185
The classification error rate on the validation data is 0.087000

0.430185

7.
The simplest form of regularization is early stopping: we use the weights as they were when validation data loss was lowest. You'll see in the plot that that is not at the end of the 1000 optimization iterations, but quite a bit earlier. The script has an option for early stopping. Run the experiment with the early stopping parameter set to true. Now the generalization should be better.
What is the validation data classification loss now, i.e. with early stopping?

validation data is 0.334505
The classification error rate on the validation data is 0.095000


8.
Another regularization method is weight decay. Let's turn off early stopping, and instead investigate weight decay. The script has an option for L2 weight decay. As long as the coefficient is 0, in effect there is no weight decay, but let's try some different coefficients.
We've already run the experiment with WD=0. Run additional experiments with the other WD coefficients listed below, and indicate which of them gave the best generalization. Be careful to focus on the classification loss (i.e. without the weight decay loss), as opposed to the final loss (which does include the weight decay loss).

0.001

0
validation data is 0.430185
The classification error rate on the validation data is 0.087000
0.0001
classification loss (i.e. without weight decay) on the validation data is 0.348294
The classification error rate on the validation data is 0.085000

0.001
classification loss (i.e. without weight decay) on the validation data is 0.287910
The classification error rate on the validation data is 0.090000

0.01
classification loss (i.e. without weight decay) on the validation data is 0.509763
The classification error rate on the validation data is 0.091000
1
classification loss (i.e. without weight decay) on the validation data is 2.302585
The classification error rate on the validation data is 0.900000
5
classification loss (i.e. without weight decay) on the validation data is 2.302585
The classification error rate on the validation data is 0.900000

9.
Yet another regularization strategy is reducing the number of model parameters, so that the model simply doesn't have the brain capacity to overfit a lot by learning too many details of the training set. In our case, we can vary the number of hidden units. Since it's clear that our model is overfitting, we'll look into reducing the number of hidden units.
Turn off the weight decay, and instead try the following hidden layer sizes. Indicate which one worked best.

30

10
0.389471 / 0.100333
30
0.364651 /  0.087222
100
0.408845
The classification error rate on the test data is 0.086444
130
0.418396
The classification error rate on the test data is 0.088667
200
0.464988
The classification error rate on the test data is 0.093778


10.
Most regularization methods can be combined quite well. Let's combine early stopping with a carefully chosen hidden layer size. Which number of hidden units works best that way, i.e. with early stopping? Remember, best, here, is based on only the validation data loss.

37

18
0.306083
The classification error rate on the validation data is 0.092000
37
0.265165
The classification error rate on the validation data is 0.081000
113
0.313749
The classification error rate on the validation data is 0.093000
189
0.315275
The classification error rate on the validation data is 0.097000
236
0.343841
The classification error rate on the validation data is 0.095000

11.
Of course, we could explore a lot more, such as maybe combining all 3 regularization methods, and that might work a little better. If you want to, you can play with the code all you want. You could even try to modify it to have 2 hidden layers, to add dropout, or anything else. The code is a reasonably well-written starting point for Neural Network experimentation. All of that, however, is beyond the scope of this assignment; here, we have only one question left.Now that we've quite carefully established a good optimization strategy as well as a good regularization strategy, it's time to see how well our model does on the task that we really cared about: reading handwritten digits.
For the settings that you chose on the previous question, what is the test data classification error rate?




0.094111





