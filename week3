1.
Which of the following neural networks are examples of a feed-forward neural network?
two

2.
consider a neural network with only one training case with input x=(x1,x2,…,xn)⊤ and correct output t. There is only one output neuron, which is logistic, i.e. y=σ(w⊤x) (notice that there are no biases). The loss function is squared error. The network has no hidden units, so the inputs are directly connected to the output neuron with weights w=(w1,w2,…,wn)⊤. We're in the process of training the neural network with the backpropagation algorithm. What will the algorithm add to wi for the next iteration if we use a step size (also known as a learning rate) of ϵ?
ϵ(t−y)y(1−y)xi

3.
Suppose we have a set of examples and Brian comes in and duplicates every example, then randomly reorders the examples. We now have twice as many examples, but no more information about the problem than we had before. If we do not remove the duplicate entries, which one of the following methods will not be affected by this change, in terms of the computer time (time in seconds, for example) it takes to come close to convergence?

Online learning, where for every iteration we randomly pick a training case.

4.
Consider a linear output unit versus a logistic output unit for a feed-forward network with no hidden layer shown below. The network has a set of inputs x and an output neuron y connected to the input by weights w and bias b.
We're using the squared error cost function even though the task that we care about, in the end, is binary classification. At training time, the target output values are 1 (for one class) and 0 (for the other class). At test time we will use the classifier to make decisions in the standard way: the class of an input x according to our model after training is as follows:
Note that we will be training the network using y, but that the decision rule shown above will be the same at test time, regardless of the type of output neuron we use for training.
Which of the following statements is true?

Unlike a linear unit, using a logistic unit will not penalize is for getting things right too confidently.

5.
Consider a neural network with one layer of logistic hidden units (intended to be fully connected to the input units) and a linear output unit. Suppose there are n input units and m hidden units. Which of the following statements are true? Check all that apply.

X As long as m≥1, this network can learn to compute any function that can be learned by a network without any hidden layers (with the same inputs).
(If the weights into the hidden layer are very small, and the weights out of it are large (to compensate), then the hidden units behave like linear units, which makes lots of things possible.)


If m>n, this network can learn more functions than if m is less than n (with n being the same).
A network with m>n has more learnable parameters than a network with m≤n (for a fixed value of n).

5.
Consider a neural network with one layer of linear hidden units (intended to be fully connected to the input units) and a logistic output unit. Suppose there are n input units and m hidden units. Which of the following statements are true? Check all that apply

X A network with m>n can learn functions that a network with m≤n cannot learn.(Linear hidden units don't add modeling capacity to the network.)

Any function that can be computed by such a network can also be computed by a network without a hidden layer.
A network with m>n has more learnable parameters than a network with m≤n (for a fixed value of n).

6.
Brian wants to make his feed-forward network (with no hidden units) using a logistic output neuron more powerful. He decides to combine the predictions of two networks by averaging them. The first network has weights w1 and the second network has weights w2. The predictions of this network for an example x are therefore:

y = 0.5(1/(1+exp(-z1))) + 0.5(1/(1+exp(-z2))),
 where z1 = w1.T*x, z2 = w2.T * x 

Can we get the exact same predictions as this combination of networks by using a single feed-forward network (again with no hidden units) using a logistic output neuron and weights w3=0.5(w1+w2)?

No


Assignment1

1. True or false: if the dataset is linearly separable, then it is possible for the number of classification errors to increase during learning.
True

2.True or false: If a generously feasible region exists, then the distance between the current weight vector and any weight vector in the feasible region will monotonically decrease as the learning proceeds.

False

3.True or false: if we use a learning rate of 2, then the perceptron algorithm will always converge to a solution for linearly separable datasets.
True

4.
9






